{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3A: Additional Machine Learning Models I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the packages and the combined data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "#!conda install py-xgboost --y\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "from random import choices\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost \n",
    "\n",
    "#from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "# No warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('DF.csv')\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the features of interest based on the preliminary analysis from Part 1. Split the data into train set and test set (20%) to allow for train set to be balanced.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.loc[:,[ 'WEATHER_CONDITION',\n",
    "       'CRASH_DAY_OF_WEEK', 'CRASH_MONTH', 'CRASH_HOUR', 'LIGHTING_CONDITION',\n",
    "       'MANEUVER', 'TRAFFICWAY_TYPE', 'PRIM_CONTRIBUTORY_CAUSE',\n",
    "       'POSTED_SPEED_LIMIT','COMBINED_DANGER_SCORE']]\n",
    "\n",
    "df_train_interest, df_test_interest = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used four different techniques to address the issue of imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 122685, 2.0: 144755, 3.0: 23799}\n"
     ]
    }
   ],
   "source": [
    "# Approach 1: Upsampling or known as oversampling\n",
    "\n",
    "## do RANDOM SAMPLING TO PICK EQUAL NUMBER OF DATA IN EACH Y GROUP\n",
    "# down sampling\n",
    "df = df_train_interest\n",
    "df4=df.reset_index()\n",
    "X=df4.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "y=df4.COMBINED_DANGER_SCORE\n",
    "\n",
    "# find the number of levels in y and number of entries associated with each level\n",
    "\n",
    "unique_levels = np.unique(y)\n",
    "unique_counts = {level: sum(y == level) for level in unique_levels}\n",
    "print(unique_counts)\n",
    "\n",
    "# find the target number of data points\n",
    "unique_counts.items()\n",
    "max_level = max(unique_counts.items(), key=operator.itemgetter(1))[0]\n",
    "min_level = min(unique_counts.items(), key=operator.itemgetter(1))[0]\n",
    "target_number = unique_counts[max_level]\n",
    "target_number_min = unique_counts[min_level]\n",
    "\n",
    "# find which data points are associated with which group\n",
    "\n",
    "grouped_levels = {}\n",
    "for ii, level in enumerate(unique_levels):\n",
    "    obs_idx = [idx for idx, val in enumerate(y) if val == level]\n",
    "    grouped_levels[level] = obs_idx\n",
    "\n",
    "grouped_levels\n",
    "\n",
    "#oversampling\n",
    "sampled_levels={}\n",
    "\n",
    "# sample indices\n",
    "for i in list(unique_levels):\n",
    "    if i != max_level:\n",
    "        sampled_levels[i] = choices(grouped_levels[i], k=target_number )\n",
    "    else:\n",
    "        sampled_levels[i] = grouped_levels[i]\n",
    "\n",
    "first = df4.iloc[sampled_levels[1]].reset_index()\n",
    "second = df4.iloc[sampled_levels[2]].reset_index()\n",
    "third = df4.iloc[sampled_levels[3]].reset_index()\n",
    "\n",
    "new_oversampled = pd.concat([first,second,third], axis = 0)\n",
    "new_oversampled = new_oversampled.drop(columns = ['level_0','index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Downsampling \n",
    "\n",
    "sampled_levels={}\n",
    "\n",
    "# sample indices\n",
    "for i in list(unique_levels):\n",
    "    if i != min_level:\n",
    "        sampled_levels[i] = choices(grouped_levels[i], k=target_number_min )\n",
    "    else:\n",
    "        sampled_levels[i] = grouped_levels[i]\n",
    "\n",
    "first = df4.iloc[sampled_levels[1]].reset_index()\n",
    "second = df4.iloc[sampled_levels[2]].reset_index()\n",
    "third = df4.iloc[sampled_levels[3]].reset_index()\n",
    "\n",
    "new_downsampled = pd.concat([first,second,third], axis = 0)\n",
    "new_downsampled = new_downsampled.drop(columns = ['level_0','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_interest = new_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert categorical variables to categories for one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEATHER_CONDITION          object\n",
      "CRASH_DAY_OF_WEEK           int64\n",
      "LIGHTING_CONDITION         object\n",
      "MANEUVER                   object\n",
      "TRAFFICWAY_TYPE            object\n",
      "PRIM_CONTRIBUTORY_CAUSE    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "labels = ['WEATHER_CONDITION', 'CRASH_DAY_OF_WEEK','LIGHTING_CONDITION','MANEUVER', 'TRAFFICWAY_TYPE', 'PRIM_CONTRIBUTORY_CAUSE']\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "print(df_train_interest[labels].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into test and training datasets. While testing, remember to use the real data to obtain a more realistic result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_interest = pd.get_dummies(df_train_interest, drop_first = True)\n",
    "df_test_interest = pd.get_dummies(df_test_interest, drop_first = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we investigate the predictive accuracies of 3 additional machine learning models, namely logistic regression, xgboost, and neural networks. The predictive accuracy of each model is first estimated using 10-fold cross-validation before it is used to predict the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation accuracy for logistic regression: 0.4490207592138441\n",
      "10-fold cross validation accuracy for xgboost: 0.4529791717039135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "k = 5\n",
    "accuracy_list_log=[]\n",
    "recall_list_log = []\n",
    "precision_list_log = []\n",
    "\n",
    "accuracy_list_forest=[]\n",
    "recall_list_forest = []\n",
    "precision_list_forest = []\n",
    "\n",
    "accuracy_list_boost=[]\n",
    "recall_list_boost = []\n",
    "precision_list_boost = []\n",
    "\n",
    "#prepare features and targets for train and test sets\n",
    "    \n",
    "X_train = df_train_interest.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "Y_train = df_train_interest.COMBINED_DANGER_SCORE\n",
    "X_test  = df_test_interest.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "Y_test = df_test_interest.COMBINED_DANGER_SCORE\n",
    "\n",
    "#perform 10-fold cross validation on each model\n",
    "#Logistic Regression\n",
    "\n",
    "LogisticRegressionModel = linear_model.LogisticRegression()\n",
    "accuracy_list_log = cross_val_score(LogisticRegressionModel, X_train, Y_train, cv = k)\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "#xgboost\n",
    "\n",
    "boost = XGBClassifier()\n",
    "accuracy_list_boost = cross_val_score(boost, X_train, Y_train, cv = k)\n",
    "\n",
    "#neural networks\n",
    "\n",
    "print('10-fold cross validation accuracy for logistic regression:', np.mean(accuracy_list_log))\n",
    "print('10-fold cross validation accuracy for xgboost:', np.mean(accuracy_list_boost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of Logistic Regression: 0.342260678478231\n",
      "[0.47042153 0.17326182 0.69278485]\n",
      "[0.5053011  0.54142063 0.12972643]\n",
      "Test accuracy of XGBoost: 0.33988463123197366\n",
      "[0.45188612 0.18104263 0.71172054]\n",
      "[0.51595626 0.55413796 0.12760851]\n"
     ]
    }
   ],
   "source": [
    "# assess performance of models using test set accuracies and confusion matrices\n",
    "# Logistic Regression\n",
    "Y_true = Y_test\n",
    "LogisticRegressionModel.fit(X_train, Y_train)\n",
    "Y_pred_log = LogisticRegressionModel.predict(X_test)\n",
    "log_accuracy = accuracy_score(Y_test, Y_pred_log)\n",
    "ConfusionMatrix = pd.DataFrame(confusion_matrix(Y_true,Y_pred_log), columns = ['Predicted 1', 'Predicted 2','Predicted 3'], index = ['Actual 1', 'Actual 2','Actual 3'])\n",
    "#print ('Confusion matrix of test data is: \\n',ConfusionMatrix)\n",
    "\n",
    "recall = recall_score(Y_true, Y_pred_log, average = None)\n",
    "recall_list_log.append(np.array(recall))\n",
    "#print(\"Average recall for the 3 classes is - \", recall)\n",
    "\n",
    "precision = precision_score(Y_true, Y_pred_log, average = None)\n",
    "precision_list_log.append(np.array(precision))\n",
    "#print(\"Average precision for the 3 classes is - \", precision)\n",
    "\n",
    "# XGBoost\n",
    "boost.fit(X_train, Y_train)\n",
    "Y_pred = boost.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "boost_accuracy = accuracy_score(Y_test, predictions)\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) \n",
    "\n",
    "boost_confusionMatrix = pd.DataFrame(confusion_matrix(Y_true,Y_pred), columns = ['Predicted 1', 'Predicted 2','Predicted 3'], index = ['Actual 1', 'Actual 2','Actual 3'])\n",
    "#print ('Confusion matrix of test data using random forest is: \\n',boost_confusionMatrix )\n",
    "\n",
    "recall = recall_score(Y_true, Y_pred, average = None)\n",
    "recall_list_boost.append(np.array(recall))\n",
    "precision = precision_score(Y_true, Y_pred, average = None)\n",
    "precision_list_boost.append(np.array(precision))\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "\n",
    "print('Test accuracy of Logistic Regression:', log_accuracy)\n",
    "print(np.mean(recall_list_log, axis = 0))\n",
    "print(np.mean(precision_list_log, axis = 0))\n",
    "\n",
    "print('Test accuracy of XGBoost:', boost_accuracy)\n",
    "print(np.mean(recall_list_boost, axis = 0))\n",
    "print(np.mean(precision_list_boost, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Approach 3: SMOTE\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "labels = ['WEATHER_CONDITION', 'CRASH_DAY_OF_WEEK','LIGHTING_CONDITION','MANEUVER', 'TRAFFICWAY_TYPE', 'PRIM_CONTRIBUTORY_CAUSE']\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[labels] = df[labels].astype('category')\n",
    "df = pd.get_dummies(df, drop_first = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33806428792501214\n",
      "[0.45988791 0.17775793 0.65612071]\n",
      "[0.50262513 0.53017184 0.13096915]\n",
      "0.5430870584073927\n",
      "[0.53738991 0.61067441 0.20077878]\n",
      "[0.52568922 0.57672516 0.34751474]\n",
      "0.5259539284608923\n",
      "[0.34763811 0.77168379 0.        ]\n",
      "[0.53724326 0.5218363  0.        ]\n"
     ]
    }
   ],
   "source": [
    "accuracy_list_log=[]\n",
    "recall_list_log = []\n",
    "precision_list_log = []\n",
    "\n",
    "accuracy_list_forest=[]\n",
    "recall_list_forest = []\n",
    "precision_list_forest = []\n",
    "\n",
    "accuracy_list_boost=[]\n",
    "recall_list_boost = []\n",
    "precision_list_boost = []\n",
    "\n",
    "for i in range(0,20):\n",
    "    train_df1, test_df1 = train_test_split(df, test_size=0.2)\n",
    "    X_train_SMOTE = train_df1.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "    Y_train_SMOTE = train_df1.COMBINED_DANGER_SCORE\n",
    "    X_test_SMOTE = test_df1.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "    Y_test_SMOTE = test_df1.COMBINED_DANGER_SCORE\n",
    "\n",
    "    X_resampled_SMOTE, Y_resampled_SMOTE = SMOTE().fit_sample(X_train_SMOTE, Y_train_SMOTE)\n",
    "    #print(sorted(Counter(Y_resampled_SMOTE).items()))\n",
    "\n",
    "    LogisticRegressionModel.fit(X_resampled_SMOTE,Y_resampled_SMOTE)\n",
    "    log_accuracy = LogisticRegressionModel.score(X_test_SMOTE,Y_test_SMOTE)\n",
    "    accuracy_list_log.append(log_accuracy)\n",
    "    #print(\"Logistic accuracy: %.2f%%\" % (log_accuracy * 100.0))\n",
    "\n",
    "    Y_true_SMOTE = Y_test_SMOTE\n",
    "    Y_pred_SMOTE = LogisticRegressionModel.predict(X_test_SMOTE)\n",
    "    Log_ConfusionMatrix = pd.DataFrame(confusion_matrix(Y_true_SMOTE,Y_pred_SMOTE), columns = ['Predicted 1', 'Predicted 2','Predicted 3'], index = ['Actual 1', 'Actual 2','Actual 3'])\n",
    "    #print ('Confusion matrix of test data using Logistic regression is: \\n',Log_ConfusionMatrix)\n",
    "\n",
    "    recall = recall_score(Y_true_SMOTE, Y_pred_SMOTE, average = None)\n",
    "    recall_list_log.append(np.array(recall))\n",
    "    precision = precision_score(Y_true_SMOTE, Y_pred_SMOTE, average = None)\n",
    "    precision_list_log.append(np.array(precision))\n",
    "    \n",
    "    #print(\"Average recall for the 3 classes using Logistic regression  is - \", recall_score(Y_true_SMOTE,Y_pred_SMOTE, average = None))\n",
    "    #print(\"Average precision for the 3 classes using Logistic regression is - \", precision_score(Y_true_SMOTE,Y_pred_SMOTE, average = None))\n",
    "\n",
    "    # random forest\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\n",
    "    classifier.fit(X_train_SMOTE, Y_train_SMOTE)\n",
    "    Y_pred_SMOTE_forest = classifier.predict(X_test_SMOTE)\n",
    "    Y_true_SMOTE_forest = Y_test_SMOTE\n",
    "\n",
    "\n",
    "    forest_accuracy = classifier.score(X_test_SMOTE,Y_test_SMOTE)\n",
    "    accuracy_list_forest.append(forest_accuracy)\n",
    "    #print(\"Random Forest accuracy: %.2f%%\" % (forest_accuracy * 100.0))\n",
    "\n",
    "    forest_confusionMatrix = pd.DataFrame(confusion_matrix(Y_true_SMOTE_forest,Y_pred_SMOTE_forest), columns = ['Predicted 1', 'Predicted 2','Predicted 3'], index = ['Actual 1', 'Actual 2','Actual 3'])\n",
    "    #print ('Confusion matrix of test data using random forest is: \\n',forest_confusionMatrix )\n",
    "    recall = recall_score(Y_true_SMOTE_forest, Y_pred_SMOTE_forest, average = None)\n",
    "    recall_list_forest.append(np.array(recall))\n",
    "    #print(\"Average recall for the 3 classes is - \", recall)\n",
    "    \n",
    "    precision = precision_score(Y_true_SMOTE_forest, Y_pred_SMOTE_forest, average = None)\n",
    "    precision_list_forest.append(np.array(precision))\n",
    "    #print(\"Average recall for the 3 classes using Logistic regression  is - \", recall_score(Y_true_SMOTE_forest,Y_pred_SMOTE_forest, average = None))\n",
    "    #print(\"Average precision for the 3 classes using Logistic regression is - \", precision_score(Y_true_SMOTE_forest,Y_pred_SMOTE_forest, average = None))\n",
    "    \n",
    "    #xgboost\n",
    "    import os\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train_SMOTE, Y_train_SMOTE)\n",
    "\n",
    "    Y_pred = model.predict(X_test_SMOTE)\n",
    "    predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # evaluate predictions\n",
    "    boost_accuracy = accuracy_score(Y_test_SMOTE, predictions)\n",
    "    accuracy_list_boost.append(boost_accuracy)\n",
    "    #print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) \n",
    "\n",
    "    boost_confusionMatrix = pd.DataFrame(confusion_matrix(Y_true,Y_pred), columns = ['Predicted 1', 'Predicted 2','Predicted 3'], index = ['Actual 1', 'Actual 2','Actual 3'])\n",
    "    #print ('Confusion matrix of test data using random forest is: \\n',boost_confusionMatrix )\n",
    "    \n",
    "    recall = recall_score(Y_true_SMOTE, Y_pred, average = None)\n",
    "    recall_list_boost.append(np.array(recall))\n",
    "    precision = precision_score(Y_true_SMOTE, Y_pred, average = None)\n",
    "    precision_list_boost.append(np.array(precision))\n",
    "\n",
    "    \n",
    "print(np.mean(accuracy_list_log))\n",
    "print(np.mean(recall_list_log, axis = 0))\n",
    "print(np.mean(precision_list_log, axis = 0))\n",
    "\n",
    "print(np.mean(accuracy_list_forest))\n",
    "print(np.mean(recall_list_forest, axis = 0))\n",
    "print(np.mean(precision_list_forest, axis = 0))\n",
    "\n",
    "print(np.mean(accuracy_list_boost))\n",
    "print(np.mean(recall_list_boost, axis = 0))\n",
    "print(np.mean(precision_list_boost, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 4: ADASYN\n",
    "\n",
    "# sm = ADASYN()\n",
    "# X_resampled_ADASYN, Y_resampled_ADASYN = sm.fit_sample(X_train, Y_train)\n",
    "# print(sorted(Counter(Y_resampled_ADASYN).items()))\n",
    "\n",
    "\n",
    "# LogisticRegressionModel.fit(X_resampled_ADASYN,Y_resampled_ADASYN)\n",
    "# print(LogisticRegressionModel.score(X_test,Y_test))\n",
    "\n",
    "# Y_true = Y_test\n",
    "# Y_pred = LogisticRegressionModel.predict(X_test)\n",
    "# ConfusionMatrix = pd.DataFrame(confusion_matrix(Y_true,Y_pred), columns = ['Predicted 1', 'Predicted 2','Predicted 3'], index = ['Actual 1', 'Actual 2','Actual 3'])\n",
    "# ConfusionMatrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
