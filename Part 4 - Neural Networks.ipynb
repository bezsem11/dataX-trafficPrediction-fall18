{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we consider the various resampling techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "df = pd.read_csv('DF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section picks the columns of interest for the statistical modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.loc[:,[ 'WEATHER_CONDITION',\n",
    "       'CRASH_DAY_OF_WEEK', 'CRASH_MONTH', 'CRASH_HOUR', 'LIGHTING_CONDITION',\n",
    "       'MANEUVER', 'TRAFFICWAY_TYPE', 'PRIM_CONTRIBUTORY_CAUSE',\n",
    "       'POSTED_SPEED_LIMIT','COMBINED_DANGER_SCORE']]\n",
    "\n",
    "df.head()\n",
    "df_test_interest = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used four different techniques to address the issue of imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 153383, 2.0: 180741, 3.0: 29925}\n"
     ]
    }
   ],
   "source": [
    "# Approach 1: Upsampling or known as oversampling\n",
    "\n",
    "## do RANDOM SAMPLING TO PICK EQUAL NUMBER OF DATA IN EACH Y GROUP\n",
    "# down sampling\n",
    "df4=df.reset_index()\n",
    "X=df4.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "y=df4.COMBINED_DANGER_SCORE\n",
    "\n",
    "# find the number of levels in y and number of entries associated with each level\n",
    "\n",
    "unique_levels = np.unique(y)\n",
    "unique_counts = {level: sum(y == level) for level in unique_levels}\n",
    "print(unique_counts)\n",
    "\n",
    "# find the target number of data points\n",
    "unique_counts.items()\n",
    "max_level = max(unique_counts.items(), key=operator.itemgetter(1))[0]\n",
    "min_level = min(unique_counts.items(), key=operator.itemgetter(1))[0]\n",
    "target_number = unique_counts[max_level]\n",
    "target_number_min = unique_counts[min_level]\n",
    "\n",
    "# find which data points are associated with which group\n",
    "\n",
    "grouped_levels = {}\n",
    "for ii, level in enumerate(unique_levels):\n",
    "    obs_idx = [idx for idx, val in enumerate(y) if val == level]\n",
    "    grouped_levels[level] = obs_idx\n",
    "\n",
    "grouped_levels\n",
    "\n",
    "#oversampling\n",
    "sampled_levels={}\n",
    "\n",
    "# sample indices\n",
    "for i in list(unique_levels):\n",
    "    if i != max_level:\n",
    "        sampled_levels[i] = choices(grouped_levels[i], k=target_number )\n",
    "    else:\n",
    "        sampled_levels[i] = grouped_levels[i]\n",
    "\n",
    "first = df4.iloc[sampled_levels[1]].reset_index()\n",
    "second = df4.iloc[sampled_levels[2]].reset_index()\n",
    "third = df4.iloc[sampled_levels[3]].reset_index()\n",
    "\n",
    "new_oversampled = pd.concat([first,second,third], axis = 0)\n",
    "new_oversampled = new_oversampled.drop(columns = ['level_0','index'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Approach 2: Downsampling \n",
    "\n",
    "sampled_levels={}\n",
    "from random import choices\n",
    "# sample indices\n",
    "for i in list(unique_levels):\n",
    "    if i != min_level:\n",
    "        sampled_levels[i] = choices(grouped_levels[i], k=target_number_min )\n",
    "    else:\n",
    "        sampled_levels[i] = grouped_levels[i]\n",
    "\n",
    "first = df4.iloc[sampled_levels[1]].reset_index()\n",
    "second = df4.iloc[sampled_levels[2]].reset_index()\n",
    "third = df4.iloc[sampled_levels[3]].reset_index()\n",
    "\n",
    "new_downsampled = pd.concat([first,second,third], axis = 0)\n",
    "new_downsampled = new_downsampled.drop(columns = ['level_0','index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'df_train_interest' is either 'new_oversampled' or 'new_downsampled', based on whether we are considering the oversampled dataframe or the undersampled dataframe respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_interest = new_oversampled\n",
    "df_train_interest = new_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert categorical variables to categories for one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEATHER_CONDITION          category\n",
      "CRASH_DAY_OF_WEEK          category\n",
      "LIGHTING_CONDITION         category\n",
      "MANEUVER                   category\n",
      "TRAFFICWAY_TYPE            category\n",
      "PRIM_CONTRIBUTORY_CAUSE    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define the lambda function: categorize_label\n",
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "labels = ['WEATHER_CONDITION', 'CRASH_DAY_OF_WEEK','LIGHTING_CONDITION','MANEUVER', 'TRAFFICWAY_TYPE', 'PRIM_CONTRIBUTORY_CAUSE']\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df_train_interest[labels] = df_train_interest[labels].astype('category')\n",
    "print(df_train_interest[labels].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test_interest[labels] = df_test_interest[labels].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_interest = pd.get_dummies(df_train_interest, drop_first = True)\n",
    "df_test_interest = pd.get_dummies(df_test_interest, drop_first = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning algorithm\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df1, test_df1 = train_test_split(df_train_interest, test_size=0.2, random_state=100)\n",
    "train_df2, test_df2 = train_test_split(df_test_interest, test_size=0.2, random_state=100)\n",
    "#train_df.columns\n",
    "\n",
    "X_train = train_df1.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "Y_train = train_df1.COMBINED_DANGER_SCORE\n",
    "X_test  = test_df2.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "Y_test = test_df2.COMBINED_DANGER_SCORE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we can run the Neural Network Model (changing the dataframe to be from the unsampling or downsampling methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71820,)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71820, 63)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # no. of columns for X_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neural network model, 3 layers \n",
    "NNmodel = Sequential()\n",
    "NNmodel.add(Dense(10, input_dim=63, activation='relu'))\n",
    "# creating the first layer with the input_dim argument and setting it to 63 for the 63 input variables\n",
    "NNmodel.add(Dense(63, activation='relu'))\n",
    "NNmodel.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_map = {1:0, 2:1, 3:2} #Â mapping values for Y_train\n",
    "Y_train=Y_train.map(Y_train_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_map = {1:0, 2:1, 3:2} # mapping values for Y_test\n",
    "Y_test=Y_test.map(Y_test_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical one-hot encoding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y_train_cat = to_categorical(Y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_cat = to_categorical(Y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "71820/71820 [==============================] - 22s 311us/step - loss: 1.0601 - categorical_accuracy: 0.4208\n",
      "Epoch 2/5\n",
      "71820/71820 [==============================] - 18s 253us/step - loss: 1.0437 - categorical_accuracy: 0.4417\n",
      "Epoch 3/5\n",
      "71820/71820 [==============================] - 18s 254us/step - loss: 1.0417 - categorical_accuracy: 0.4452\n",
      "Epoch 4/5\n",
      "71820/71820 [==============================] - 19s 263us/step - loss: 1.0403 - categorical_accuracy: 0.4466\n",
      "Epoch 5/5\n",
      "71820/71820 [==============================] - 19s 268us/step - loss: 1.0398 - categorical_accuracy: 0.4478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4234e518>"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 10 samples\n",
    "NNmodel.fit(X_train, Y_train_cat, epochs=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72810/72810 [==============================] - 4s 52us/step\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using X_test and Y_test\n",
    "scores = NNmodel.evaluate(X_test, Y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "categorical_accuracy: 33.23%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n%s: %.2f%%\" % (NNmodel.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 2, 0])"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.asarray(pd.get_dummies(Y_pred_nn.argmax(axis = 1)))\n",
    "np.asarray(Y_pred_nn.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix of test data is: \n",
      "           Predicted 1  Predicted 2  Predicted 3\n",
      "Actual 1        12778         5489        12556\n",
      "Actual 2        10899         7055        18106\n",
      "Actual 3          878          684         4365\n"
     ]
    }
   ],
   "source": [
    "#Y_true_nn = Y_test_cat\n",
    "Y_true_nn = Y_test\n",
    "\n",
    "Y_pred_nn = NNmodel.predict(X_test)\n",
    "\n",
    "ConfusionMatrix = pd.DataFrame(confusion_matrix(Y_true_nn,np.asarray(Y_pred_nn.argmax(axis = 1))), columns = ['Predicted 1', 'Predicted 2','Predicted 3'], index = ['Actual 1', 'Actual 2','Actual 3'])\n",
    "print ('Confusion matrix of test data is: \\n',ConfusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41456056 0.19564615 0.73646027]\n",
      "[0.52038281 0.53333837 0.12461815]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(recall_score(Y_test_cat,np.asarray(pd.get_dummies(Y_pred_nn.argmax(axis = 1))),average=None))\n",
    "print(precision_score(Y_test_cat,np.asarray(pd.get_dummies(Y_pred_nn.argmax(axis = 1))),average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SMOTE as a resampling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 3: SMOTE\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "labels = ['WEATHER_CONDITION', 'CRASH_DAY_OF_WEEK','LIGHTING_CONDITION','MANEUVER', 'TRAFFICWAY_TYPE', 'PRIM_CONTRIBUTORY_CAUSE']\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[labels] = df[labels].astype('category')\n",
    "df = pd.get_dummies(df, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1, test_df1 = train_test_split(df, test_size=0.2)\n",
    "X_train_SMOTE = train_df1.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "Y_train_SMOTE = train_df1.COMBINED_DANGER_SCORE\n",
    "X_test_SMOTE = test_df1.drop(columns = 'COMBINED_DANGER_SCORE')\n",
    "Y_test_SMOTE = test_df1.COMBINED_DANGER_SCORE\n",
    "\n",
    "X_resampled_SMOTE, Y_resampled_SMOTE = SMOTE().fit_sample(X_train_SMOTE, Y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_resampled_SMOTE_map = {1:0, 2:1, 3:2}\n",
    "Y_resampled_SMOTE=pd.Series(Y_resampled_SMOTE).map(Y_resampled_SMOTE_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_resampled_SMOTE_cat = to_categorical(Y_resampled_SMOTE, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "433218/433218 [==============================] - 120s 278us/step - loss: 0.9297 - categorical_accuracy: 0.5130\n",
      "Epoch 2/5\n",
      "433218/433218 [==============================] - 129s 297us/step - loss: 0.9070 - categorical_accuracy: 0.5244\n",
      "Epoch 3/5\n",
      "433218/433218 [==============================] - 124s 286us/step - loss: 0.9040 - categorical_accuracy: 0.5271\n",
      "Epoch 4/5\n",
      "433218/433218 [==============================] - 133s 306us/step - loss: 0.9022 - categorical_accuracy: 0.5279\n",
      "Epoch 5/5\n",
      "433218/433218 [==============================] - 142s 327us/step - loss: 0.9007 - categorical_accuracy: 0.5287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3141e748>"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNmodel.fit(X_resampled_SMOTE, Y_resampled_SMOTE_cat, epochs=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72810/72810 [==============================] - 4s 51us/step\n",
      "\n",
      "categorical_accuracy: 49.31%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using X_test and Y_test\n",
    "scores = NNmodel.evaluate(X_test, Y_test_cat)\n",
    "print(\"\\n%s: %.2f%%\" % (NNmodel.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix of test data is: \n",
      "           Predicted 1  Predicted 2  Predicted 3\n",
      "Actual 1        12456        16363         2004\n",
      "Actual 2        10568        22679         2813\n",
      "Actual 3          887         4270          770\n"
     ]
    }
   ],
   "source": [
    "Y_true_nn = Y_test\n",
    "Y_pred_nn = NNmodel.predict(X_test)\n",
    "\n",
    "ConfusionMatrix = pd.DataFrame(confusion_matrix(Y_true_nn,np.asarray(Y_pred_nn.argmax(axis = 1))), columns = ['Predicted 1', 'Predicted 2','Predicted 3'], index = ['Actual 1', 'Actual 2','Actual 3'])\n",
    "print ('Confusion matrix of test data is: \\n',ConfusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40411381 0.62892402 0.12991395]\n",
      "[0.52093179 0.52361932 0.13781994]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(recall_score(Y_test_cat,np.asarray(pd.get_dummies(Y_pred_nn.argmax(axis = 1))),average=None))\n",
    "print(precision_score(Y_test_cat,np.asarray(pd.get_dummies(Y_pred_nn.argmax(axis = 1))),average=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
